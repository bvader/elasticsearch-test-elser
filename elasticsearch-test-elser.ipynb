{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bvader/elasticsearch-test-elser/blob/main/elasticsearch-test-elser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "ddI9pmHfaQHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install elasticsearch"
      ],
      "metadata": {
        "id": "cRUzDQ2MtIna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in connection and auth info\n",
        "# Note the port is REQUIRED for the elasticsearch endpoint!\n",
        "import getpass, os\n",
        "\n",
        "os.environ['es_url'] = getpass.getpass('Enter Elasticsearch Endpoint:  ')\n",
        "os.environ['es_user'] = getpass.getpass('Enter User:  ')\n",
        "os.environ['es_pwd'] = getpass.getpass('Enter Password:  ')"
      ],
      "metadata": {
        "id": "dnbLkZRjHenj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbqVOuPbsxb_"
      },
      "outputs": [],
      "source": [
        "# Connect and test connection\n",
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "\n",
        "es_url = os.environ['es_url']\n",
        "es_user = os.environ['es_user']\n",
        "es_pwd = os.environ['es_pwd']\n",
        "\n",
        "# Initialize the Elasticsearch client\n",
        "es = Elasticsearch(\n",
        "    [es_url],\n",
        "    basic_auth=(es_user, es_pwd),\n",
        "    request_timeout=30\n",
        ")\n",
        "es.info().body"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data and Model Setup"
      ],
      "metadata": {
        "id": "BdDIdAgsaXhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See https://registry.opendata.aws/amazon-pqa/\n",
        "# See https://amazon-pqa.s3.amazonaws.com/readme.txt\n",
        "# aws s3 ls --no-sign-request s3://amazon-pqa/\n",
        "# https://amazon-pqa.s3.amazonaws.com/amazon-pqa.tar.gz\n",
        "\n",
        "# Upload the file first\n",
        "!head /content/sample_data/amazon_pqa_headset.json"
      ],
      "metadata": {
        "id": "y6e_aa9kyjcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data file (Just load 1000 for now)\n",
        "\n",
        "import sys\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm\n",
        "\n",
        "from elasticsearch import Elasticsearch\n",
        "from elasticsearch.helpers import bulk\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "df = pd.DataFrame(columns=('question', 'answer'))\n",
        "\n",
        "with open('/content/sample_data/amazon_pqa_headset.json') as f:\n",
        "    i=0\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        df.loc[i] = [data['question_text'],data['answers'][0]['answer_text']]\n",
        "        i+=1\n",
        "        if(i == 1000):\n",
        "            break\n",
        "\n",
        "print(df.columns)\n",
        "print(df.shape)\n",
        "print(df.iloc[3]['question'])\n",
        "print(df.iloc[3]['answer'])"
      ],
      "metadata": {
        "id": "ktH56r5qvOi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Index, Pipeline and Load Index"
      ],
      "metadata": {
        "id": "LvdnSx7xapUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates an index in Elasticsearch with raw data\n",
        "es.options(ignore_status=400).indices.create(\n",
        "    index=\"nlp_pqa_1000\",\n",
        "    settings={\"number_of_shards\": 1},\n",
        "    mappings={\n",
        "        \"properties\": {\n",
        "            \"question\": { \"type\": \"text\"},\n",
        "            \"answer\": {\"type\": \"text\"},\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "OOHQYK38vvZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Action\n",
        "def generator():\n",
        "    for index, row in df.iterrows():\n",
        "        yield {\n",
        "            \"_index\": \"nlp_pqa_1000\",\n",
        "            \"question\": row[\"question\"],\n",
        "            \"answer\": row[\"answer\"]\n",
        "        }\n",
        "# Bulk indexing nlp_\n",
        "try:\n",
        "    res = bulk(es, generator())\n",
        "    print(\"Response: \", res)\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "3DMU1zVY_nqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download / Load ELSER\n",
        "es.ml.put_trained_model(model_id=\".elser_model_1\", input={\"field_names\": \"text_field\"})"
      ],
      "metadata": {
        "id": "uu-gz8tI-8vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start ELSER\n",
        "es.ml.start_trained_model_deployment(\n",
        "    model_id=\".elser_model_1\"\n",
        ")"
      ],
      "metadata": {
        "id": "RXv8KvjigR57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Elser Pipeline\n",
        "es.ingest.put_pipeline(id=\"elser-expansion\",\n",
        "    description=\"Elser pipeline\",\n",
        "    processors=[\n",
        "    {\n",
        "        \"inference\": {\n",
        "        \"model_id\": \".elser_model_1\",\n",
        "        # This maps the fields between the model input and the input index\n",
        "        # in this case text_field is the field that the model uses to vectorize\n",
        "        # question is the field in the source index to be vectorized\n",
        "        # so this says use the question field as the input to the model\n",
        "        \"field_map\": {\n",
        "            \"question\": \"text_field\"\n",
        "        },\n",
        "        \"target_field\": \"ml\",\n",
        "        \"inference_config\": {\n",
        "          \"text_expansion\": {\n",
        "            \"results_field\": \"tokens\"\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  on_failure=[\n",
        "    {\n",
        "      \"set\": {\n",
        "        \"description\": \"Index document to 'failed-<index>'\",\n",
        "        \"field\": \"_index\",\n",
        "        \"value\": \"failed-{{{_index}}}\"\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"set\": {\n",
        "        \"description\": \"Set error message\",\n",
        "        \"field\": \"ingest.failure\",\n",
        "        \"value\": \"{{_ingest.on_failure_message}}\"\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "sO3ZPDrZCPcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index using Elser expansion\n",
        "es.options(ignore_status=400).indices.create(\n",
        "    index=\"nlp_pqa_1000_elser_embeddings\",\n",
        "    settings={\"number_of_shards\": 1, \"index.mapping.total_fields.limit\": 2000 },\n",
        "    mappings={\n",
        "        \"properties\": {\n",
        "            \"ml.tokens\": {\n",
        "                \"type\": \"rank_features\"\n",
        "            },\n",
        "            \"question\": { \"type\": \"text\"},\n",
        "            \"answer\": {\"type\": \"text\"}\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "80j3zYvcFcwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data with term expansion\n",
        "def generator():\n",
        "    for index, row in df.iterrows():\n",
        "        yield {\n",
        "            \"_index\": \"nlp_pqa_1000_elser_embeddings\",\n",
        "            \"pipeline\": \"elser-expansion\",\n",
        "            \"question\": row[\"question\"],\n",
        "            \"answer\": row[\"answer\"]\n",
        "        }\n",
        "\n",
        "try:\n",
        "    res = bulk(es, generator(), chunk_size=100) # batch size 100\n",
        "    print(\"Response: \", res)\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "GhyIGRDzG_nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Queries"
      ],
      "metadata": {
        "id": "PNP_u7B4a0pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Text Expansion\n",
        "\n",
        "query_text = input(\"Enter a question :\")\n",
        "print('\\n')\n",
        "\n",
        "query={\n",
        "    \"text_expansion\": {\n",
        "    \"ml.tokens\": {\n",
        "        \"model_id\":\".elser_model_1\",\n",
        "        \"model_text\": query_text\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "resp = es.search(index=\"nlp_pqa_1000_elser_embeddings\", query=query)\n",
        "\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    score = hit['_score']\n",
        "    question = hit['_source']['question']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"Question: {question}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "B3YXTTzzLUPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text expansion with filter, exclude results\n",
        "# search for \"Does this work with xbox\"\n",
        "\n",
        "query_text = input(\"Enter a question :\")\n",
        "print('\\n')\n",
        "\n",
        "query={\n",
        "    \"bool\": {\n",
        "      \"must\": [\n",
        "        {\n",
        "          \"text_expansion\": {\n",
        "            \"ml.tokens\": {\n",
        "              \"model_id\": \".elser_model_1\",\n",
        "              \"model_text\": query_text\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      ],\n",
        "      \"must_not\": [\n",
        "        {\n",
        "         \"match_phrase\": {\n",
        "            \"question\": \"xbox one\"\n",
        "          }\n",
        "        },\n",
        "        {\n",
        "          \"match_phrase\": {\n",
        "            \"question\": \"xbox 1\"\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "}\n",
        "\n",
        "resp = es.search(index=\"nlp_pqa_1000_elser_embeddings\", query=query)\n",
        "\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    score = hit['_score']\n",
        "    question = hit['_source']['question']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"Question: {question}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "pN01t-9P6F_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Expansion with filter... only include certain results\n",
        "# search for \"Does this work with xbox\"\n",
        "query_text = input(\"Enter a question :\")\n",
        "print('\\n')\n",
        "\n",
        "query={\n",
        "    \"bool\": {\n",
        "      \"must\": [\n",
        "        {\n",
        "          \"text_expansion\": {\n",
        "            \"ml.tokens\": {\n",
        "              \"model_id\": \".elser_model_1\",\n",
        "              \"model_text\": query_text\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      ],\n",
        "      \"must\": [\n",
        "        {\n",
        "         \"match\": {\n",
        "            \"question\": \"xbox 360\"\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "}\n",
        "\n",
        "resp = es.search(index=\"nlp_pqa_1000_elser_embeddings\", query=query)\n",
        "\n",
        "print(f\"{resp}\\n\")\n",
        "\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    score = hit['_score']\n",
        "    question = hit['_source']['question']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"Question: {question}\\nAnswer: {answer}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "i3J_qVL289Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid search with text expansion and rrf\n",
        "# NOTE : This works with a slightly lower level of the API\n",
        "# The higher level / abstracted  API is still under development\n",
        "query_text = input (\"Enter a question :\")\n",
        "\n",
        "print('\\n')\n",
        "body = {\n",
        "  \"sub_searches\": [\n",
        "    {\n",
        "      \"query\": {\n",
        "        \"bool\": {\n",
        "          \"must\": [\n",
        "          {\n",
        "            \"match\": {\n",
        "            \"answer\": \"polycom\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      }\n",
        "    },\n",
        "    {\n",
        "      \"query\": {\n",
        "        \"text_expansion\": {\n",
        "          \"ml.tokens\": {\n",
        "          \"model_id\": \".elser_model_1\",\n",
        "          \"model_text\": query_text\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  \"rank\": {\n",
        "    \"rrf\": {\n",
        "        \"window_size\": 50,\n",
        "        \"rank_constant\": 20\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "index = \"nlp_pqa_1000_elser_embeddings\"\n",
        "resp = es.perform_request(\"POST\", f\"/{index}/_search\", headers={\"content-type\": \"application/json\", \"accept\": \"application/json\"}, body=body)\n",
        "\n",
        "print(f\"\\n Resp:{resp} \\n\")\n",
        "for hit in resp['hits']['hits']:\n",
        "    doc_id = hit['_id']\n",
        "    rank = hit['_rank']\n",
        "    question = hit['_source']['question']\n",
        "    answer = hit['_source']['answer']\n",
        "    print(f\"\\nRank: {rank}\\nQuestion: {question}\\nAnswer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "pSoWWeyB5KVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
